import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from matplotlib import pyplot as plt
import os
from torch.optim.lr_scheduler import StepLR
import pickle

from google.colab import drive
drive.mount('/content/drive')

# Creating the model (Task 2)
class Backbone(nn.Module):
    def __init__(self, in_channels, out_channels, k, n_blocks):
        super(Backbone, self).__init__()

        # Requirement: N blocks
        self.n_blocks = n_blocks
        self.blocks = nn.ModuleList([nn.Sequential(
            nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, 3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(out_channels),
            nn.MaxPool2d(2, 2),
            #nn.Dropout(0.2 + 0.1 * i),
        ) for i in range(n_blocks)])

        # Requirement: SpatialAveragePool, linear layer predicting a=[a1,...,ak] with k elements from input tensor X
        self.attention = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(out_channels, k),
            nn.ReLU(),
        )

        # Requirement: K Conv layers
        self.convs = nn.ModuleList([nn.Conv2d(out_channels, out_channels, 3, padding=1) for _ in range(k)])

    def forward(self, x):
        # Pass input through N blocks
        for i in range(self.n_blocks):
            x = self.blocks[i](x)

        # Calculate the attention vector a
        a = self.attention(x)

        # Requirement: Combine K Conv layers using the attention vector a
        out = torch.zeros_like(x)
        for i, conv in enumerate(self.convs):
            out += a[:, i].view(-1, 1, 1, 1) * conv(x)

        return out


class Classifier(nn.Module):
    def __init__(self, in_channels, num_classes):
        super(Classifier, self).__init__()

        # Requirement: SpatialAveragePool
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))

        # Requirement: Classifier (linear layer)
        self.fc = nn.Linear(in_channels, num_classes)

    def forward(self, x):
        # Requirement: Takes as input the output of the last block
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x) # Pass f through the classifier
        return x

# Combining the backbone and the classifier.
class Net(nn.Module):
    def __init__(self, n_blocks, k, num_classes):
        super(Net, self).__init__()
        self.in_channels = 3 #number of input channels
        self.out_channels = 64 #number of output channels

        # Initialize the backbone and classifier
        self.backbone = Backbone(self.in_channels, self.out_channels, k, n_blocks)
        self.classifier = Classifier(self.out_channels, num_classes)

    def forward(self, x):
        # Pass input through the backbone and classifier
        x = self.backbone(x)
        x = self.classifier(x)
        return x

# Plot diagnostic learning curves
def summarize_diagnostics(history):
    # Plot training and validation loss
    plt.figure(figsize=(12, 6))
    plt.plot(history['train_loss'], label="Training Loss")
    plt.plot(history['val_loss'], label="Validation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.title("Loss Evolution")
    plt.show()

    # Plot training and validation accuracy
    plt.figure(figsize=(12, 6))
    plt.plot([acc * 100 for acc in history['train_acc']], label="Training Accuracy")
    plt.plot([acc * 100 for acc in history['val_acc']], label="Validation Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.legend()
    plt.title("Accuracy Evolution")
    plt.show()

def save_checkpoint(model, optimizer, epoch, filename):
    # Save model and optimizer state along with the current epoch
    state = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
    }
    torch.save(state, filename)

def load_checkpoint(model, optimizer, filename):
    # Load model and optimizer state along with the epoch
    checkpoint = torch.load(filename)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    return checkpoint['epoch']

# Save history data
def save_history(history, filename):
    with open(filename, 'wb') as f:
        pickle.dump(history, f)

# Load history data
def load_history(filename):
    with open(filename, 'rb') as f:
        return pickle.load(f)

def run_training_script(n_blocks=3, k=1, num_classes=10):
    # Define the data augmentation and normalization operations for the training set
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4), # Randomly crop the image with a padding of 4
        transforms.RandomHorizontalFlip(), # Randomly flip the image horizontally
        transforms.ToTensor(), # Convert the image to a PyTorch tensor
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # Normalize the image with mean and standard deviation
    ])

    # Just normalization for validation set
    transform_test = transforms.Compose([
        transforms.ToTensor(), # Convert the image to a PyTorch tensor
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # Normalize the image with mean and standard deviation
    ])

    # Read dataset and create dataloaders (Task 1)
    # Load the CIFAR-10 dataset for training, applying the data augmentation and normalization
    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    
    # Create a DataLoader for the training set, with a batch size of 64, shuffling enabled, and 2 worker threads
    trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)
    
    # Load the CIFAR-10 dataset for validation, applying only normalization
    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    
    # Create a DataLoader for the validation set, with a batch size of 64, shuffling enabled, and 2 worker threads
    testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)

    # Define the model, loss function, and optimizer (Task 3)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = Net(n_blocks, k, num_classes).to(device)
    criterion = nn.CrossEntropyLoss() # inherently applies softmax internally
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Create the scheduler for learning rate decay
    scheduler = StepLR(optimizer, step_size=60, gamma=0.5)

    # Train and validate the model (Task 4 and 5)
    epochs = 100
    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}

    save_every = 10  # Save checkpoint every 10 epochs
    checkpoint_filename = "/content/drive/MyDrive/checkpoint.pth"
    history_filename = "/content/drive/MyDrive/history.pkl"

    # Load checkpoint and history if available
    if os.path.exists(checkpoint_filename):
      start_epoch = load_checkpoint(model, optimizer, checkpoint_filename)
      print(f"Resuming training from epoch {start_epoch}")
    else:
      start_epoch = 0

    if os.path.exists(history_filename):
      history = load_history(history_filename)
      print("Loaded training history")
    else:
      history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}

    # Training loop
    for epoch in range(start_epoch, epochs):
        model.train()
        train_loss, train_correct, total = 0, 0, 0
        for inputs, targets in trainloader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            train_correct += predicted.eq(targets).sum().item()

        train_loss /= len(trainloader)
        train_acc = train_correct / total

        model.eval()
        val_loss, val_correct, total = 0, 0, 0
        with torch.no_grad():
            for inputs, targets in testloader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)

                val_loss += loss.item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                val_correct += predicted.eq(targets).sum().item()

        val_loss /= len(testloader)
        val_acc = val_correct / total

        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)

        print(f'Epoch: {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')

        # Update the learning rate using the scheduler
        scheduler.step()

        # Save checkpoint and history
        if (epoch + 1) % save_every == 0:
          save_checkpoint(model, optimizer, epoch + 1, checkpoint_filename)
          save_history(history, history_filename)

    # Plot diagnostic learning curves
    summarize_diagnostics(history)

# Entry point, run the test harness
n_blocks = 3 # Number of blocks in the backbone
k = 1 #number of convulational layers
num_classes = 10 # classifying out of 10 classes
model = Net(n_blocks, k, num_classes)
run_training_script()
